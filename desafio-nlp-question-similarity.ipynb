{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85661f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "\n",
    "# Arquivos Pythons com funções e constantes\n",
    "from qs_constants import AppConstants\n",
    "from qs_functions import load_qa_pairs, normalize_qa, remove_duplication, remove_bad_answer, get_context_files\n",
    "from qs_classes import TitleGroup, QuestionEmbedding\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import functional as t_funcional\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84d5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6120ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_dataset():\n",
    "    df_all = load_qa_pairs()\n",
    "    df_preparado = df_all[(df_all.Question.notna()) &\n",
    "                          (df_all.Answer.notna())]\n",
    "    vet_QA = df_preparado[[\"Question\", \"Answer\", \"ArticleTitle\",\"ArticleFile\"]].to_numpy()\n",
    "    vet_QA = normalize_qa(vet_QA, lowercase=False)\n",
    "    vet_QA = remove_bad_answer(vet_QA)\n",
    "    vet_QA = remove_duplication(vet_QA)\n",
    "    return vet_QA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146b8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupa_perguntas_por_assunto(vet_QA):\n",
    "    qa_agrupada_assunto = {}\n",
    "    for qa in vet_QA:\n",
    "        assunto = qa[AppConstants.COL_TITULO]\n",
    "        assunto_preparado = assunto.lower().replace(\"_\", \" \")\n",
    "\n",
    "        if assunto_preparado not in qa_agrupada_assunto:\n",
    "            grupo = TitleGroup()\n",
    "            qa_agrupada_assunto[assunto_preparado] = grupo\n",
    "        else:\n",
    "            grupo = qa_agrupada_assunto[assunto_preparado]\n",
    "\n",
    "            \n",
    "        q_embedding = QuestionEmbedding()\n",
    "        q_embedding.question = qa[AppConstants.COL_PERGUNTA]\n",
    "        q_embedding.answer = qa[AppConstants.COL_RESPOSTA]\n",
    "        \n",
    "        grupo.question_list.append(q_embedding)\n",
    "\n",
    "        # existem arquivos com nan\n",
    "        file = qa[AppConstants.COL_CONTEXT_FILE]\n",
    "        if isinstance(file, str):\n",
    "            grupo.files.add(file)\n",
    "    return qa_agrupada_assunto\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87db9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_contexto_assunto(qa_agrupada_assunto):\n",
    "    with  tqdm(total=len(qa_agrupada_assunto), position=0, desc='Carregando Assuntos...') as progress:\n",
    "        for k_grupo in qa_agrupada_assunto:    \n",
    "            progress.set_description(f\"Carregando Assunto [{k_grupo:<30}]\")\n",
    "            grupo = qa_agrupada_assunto[k_grupo]\n",
    "            for ctx_file in grupo.files:\n",
    "                file_path = os.path.join(AppConstants.CONTEXT_TEXT_PATH, ctx_file + AppConstants.CONTEXT_FILE_EXTENSION)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f_ctx:\n",
    "                    ctx_file_doc = f_ctx.read()\n",
    "                    ctx_nlp_doc = nlp(ctx_file_doc)\n",
    "                    if ctx_nlp_doc.ents:\n",
    "                        for ent in ctx_nlp_doc.ents:\n",
    "                            entidade = ent.text\n",
    "                            grupo.context_ner.add(entidade)\n",
    "            \n",
    "            progress.update(1)\n",
    "        progress.set_description(\"Assuntos carregados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252ea521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna uma lista de asssuntos que tem potencial match com a pergunta\n",
    "def get_assunto_pergunta(qa_agrupada_assunto, pergunta):\n",
    "    doc_pergunta = nlp(pergunta)\n",
    "    lista_assunto_match = []\n",
    "    \n",
    "    for k_assunto in qa_agrupada_assunto:\n",
    "        grupo = qa_agrupada_assunto[k_assunto]\n",
    "        achou = False\n",
    "        como_achou = ''\n",
    "        \n",
    "        # NER pergunta\n",
    "        if doc_pergunta.ents:\n",
    "            for entidade in doc_pergunta.ents:\n",
    "                entidade_text = entidade.text.lower()\n",
    "                for entidade_token in entidade_text.split():\n",
    "                    if entidade_token in k_assunto:\n",
    "                        achou = True\n",
    "                        como_achou = AppConstants.METODO_BUSCA_ASSUNTO_NER_PERGUNTA\n",
    "                        \n",
    "        # Prcura por nomes\n",
    "        if not achou and doc_pergunta.noun_chunks:\n",
    "            for noun in doc_pergunta.noun_chunks:\n",
    "                for n_part in noun.text.split():\n",
    "                    if n_part in k_assunto:\n",
    "                        achou = True\n",
    "                        como_achou = AppConstants.METODO_BUSCA_ASSUNTO_NOUN_PERGUNTA    \n",
    "    \n",
    "         # Lemmanization\n",
    "        if not achou:\n",
    "            doc_assunto = nlp(k_assunto)\n",
    "            for token_assunto in doc_assunto:\n",
    "                if token_assunto.is_stop:\n",
    "                    continue\n",
    "                assunto_lemma = token_assunto.lemma_.lower()\n",
    "                for token_pergunta in doc_pergunta:\n",
    "                    if token_pergunta.is_stop:\n",
    "                        continue\n",
    "                    if assunto_lemma == token_pergunta.lemma_.lower():\n",
    "                        achou = True\n",
    "                        como_achou = AppConstants.METODO_BUSCA_ASSUNTO_LEMMANIZATION\n",
    "                if not achou:\n",
    "                    for token_pergunta in doc_pergunta:\n",
    "                        if token_pergunta.is_stop:\n",
    "                            continue\n",
    "                        if stemmer.stem(token_pergunta.text) == stemmer.stem(token_assunto.text):\n",
    "                            achou = True\n",
    "                            como_achou = AppConstants.METODO_BUSCA_ASSUNTO_STEMMER    \n",
    "                    \n",
    "        # NER context file            \n",
    "        if not achou:\n",
    "            if doc_pergunta.ents:\n",
    "                for entidade in doc_pergunta.ents:\n",
    "                    entidade_text = entidade.text\n",
    "                    if entidade_text in grupo.context_ner:\n",
    "                            achou = True\n",
    "                            como_achou = AppConstants.METODO_BUSCA_ASSUNTO_NER_CONTEXTO\n",
    "                            \n",
    "        \n",
    "        if achou:\n",
    "            lista_assunto_match.append((k_assunto, como_achou))\n",
    "            \n",
    "    return lista_assunto_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7304cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dada uma lista de matchs para assunto retonra aquele que teoricamente seria o de maior peso, seguindo a ordem de prioridade por busca\n",
    "def get_assunto_match_principal(lista_assunto_match):\n",
    "    \n",
    "    def encontra_metodo_busca(lista_assunto, metodo_busca_pesquisa):\n",
    "        for (assunto, metodo_busca) in lista_assunto_match:\n",
    "            if metodo_busca == metodo_busca_pesquisa:\n",
    "                return assunto\n",
    "        return None\n",
    "    \n",
    "    assunto_encontrado = encontra_metodo_busca(lista_assunto_match, AppConstants.METODO_BUSCA_ASSUNTO_NER_PERGUNTA)\n",
    "    if assunto_encontrado:\n",
    "        return assunto_encontrado, AppConstants.METODO_BUSCA_ASSUNTO_NER_PERGUNTA\n",
    "        \n",
    "    assunto_encontrado = encontra_metodo_busca(lista_assunto_match, AppConstants.METODO_BUSCA_ASSUNTO_NOUN_PERGUNTA)\n",
    "    if assunto_encontrado:\n",
    "        return assunto_encontrado, AppConstants.METODO_BUSCA_ASSUNTO_NOUN_PERGUNTA\n",
    "    \n",
    "    assunto_encontrado = encontra_metodo_busca(lista_assunto_match, AppConstants.METODO_BUSCA_ASSUNTO_LEMMANIZATION)\n",
    "    if assunto_encontrado:\n",
    "        return assunto_encontrado, AppConstants.METODO_BUSCA_ASSUNTO_LEMMANIZATION\n",
    "    \n",
    "    assunto_encontrado = encontra_metodo_busca(lista_assunto_match, AppConstants.METODO_BUSCA_ASSUNTO_STEMMER)\n",
    "    if assunto_encontrado:\n",
    "        return assunto_encontrado, AppConstants.METODO_BUSCA_ASSUNTO_STEMMER\n",
    "    \n",
    "    assunto_encontrado = encontra_metodo_busca(lista_assunto_match, AppConstants.METODO_BUSCA_ASSUNTO_NER_CONTEXTO)\n",
    "    if assunto_encontrado:\n",
    "        return assunto_encontrado, AppConstants.METODO_BUSCA_ASSUNTO_NER_CONTEXTO\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68bf40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprime as perguntas cuja a entidade não conseguiu ser encontrada\n",
    "def teste_busca_contexto():\n",
    "    vet_QA = carrega_dataset()\n",
    "    qa_agrupada_assunto = agrupa_perguntas_por_assunto(vet_QA)\n",
    "    carrega_contexto_assunto(qa_agrupada_assunto)\n",
    "\n",
    "    total = 0\n",
    "    qtd_achou_lista = 0\n",
    "    qtd_achou_lista_errada = 0\n",
    "    qtd_achou_unico = 0\n",
    "    qtd_achou_unico_lista = 0\n",
    "    qtd_nao_achou = 0\n",
    "    qtd_achou_unico_incorreto = 0\n",
    "\n",
    "\n",
    "    estatistica_busca = {AppConstants.METODO_BUSCA_ASSUNTO_NER_PERGUNTA: 0, \n",
    "                         AppConstants.METODO_BUSCA_ASSUNTO_NOUN_PERGUNTA : 0,\n",
    "                         AppConstants.METODO_BUSCA_ASSUNTO_LEMMANIZATION : 0,\n",
    "                         AppConstants.METODO_BUSCA_ASSUNTO_STEMMER : 0,\n",
    "                         AppConstants.METODO_BUSCA_ASSUNTO_NER_CONTEXTO : 0,\n",
    "                         'PERGUNTA_SEM_CONTEXTO' : 0\n",
    "                        }\n",
    "\n",
    "    for k_assunto in qa_agrupada_assunto:\n",
    "        grupo = qa_agrupada_assunto[k_assunto]\n",
    "        for qa in grupo.question_list:\n",
    "            total += 1\n",
    "            achou = False\n",
    "            como_achou = \"\"\n",
    "\n",
    "            pergunta = qa.question\n",
    "\n",
    "            lista_assunto_match = get_assunto_pergunta(qa_agrupada_assunto, pergunta)\n",
    "\n",
    "            if len(lista_assunto_match) == 1:\n",
    "                (assunto_encontrado, metodo_busca) = lista_assunto_match[0]\n",
    "                if assunto_encontrado != k_assunto:\n",
    "                    qtd_achou_unico_incorreto += 1\n",
    "                else:\n",
    "                    qtd_achou_unico += 1\n",
    "                    estatistica_busca[metodo_busca] += 1\n",
    "\n",
    "            elif len(lista_assunto_match) > 0:\n",
    "                assunto_encontrado, metodo_busca = get_assunto_match_principal(lista_assunto_match)\n",
    "                if assunto_encontrado == k_assunto:\n",
    "                    qtd_achou_unico_lista += 1\n",
    "                    estatistica_busca[metodo_busca] += 1\n",
    "                else:\n",
    "                    achou_lista = False\n",
    "                    for (assunto_encontrado, metodo_busca) in lista_assunto_match:\n",
    "                        if assunto_encontrado != k_assunto:\n",
    "                            continue\n",
    "                        achou_lista = True\n",
    "                    if achou_lista:\n",
    "                        qtd_achou_lista += 1\n",
    "                    else:\n",
    "                        qtd_achou_lista_errada += 1\n",
    "            else:\n",
    "                print(\"Não achou \", k_assunto,\" - \", pergunta)\n",
    "                qtd_nao_achou +=1\n",
    "                for token_sem_ctx in AppConstants.TOKENS_COM_FALTA_CONTEXTO:\n",
    "                    if token_sem_ctx in pergunta:\n",
    "                        estatistica_busca['PERGUNTA_SEM_CONTEXTO'] += 1\n",
    "\n",
    "\n",
    "    print ('Total:', total)\n",
    "    print('Achou Unico', qtd_achou_unico)\n",
    "    print('Achou Unico Pela Lista através de prioridade entre métodos ', qtd_achou_unico_lista)\n",
    "    print('Achou Unico Incorreto', qtd_achou_unico_incorreto)\n",
    "    print('Achou Lista ', qtd_achou_lista)\n",
    "    print('Achou Lista Errada ', qtd_achou_lista_errada)\n",
    "    print('Não Achou', qtd_nao_achou)\n",
    "    print(estatistica_busca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04614fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_busca_contexto()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c63b63",
   "metadata": {},
   "source": [
    "### Processando as Sentenças para geração de Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fabe413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criação modelo, caminho Hugginface recebido por parametro\n",
    "def create_model(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    model.to('cuda')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b59de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liberar espaço ocupado pelo modelo na GPU\n",
    "def destroy_model(model):\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596be053",
   "metadata": {},
   "source": [
    "##### Prepara o dataset para entrar no modelo bert. Gera input_id, attention_mask e monta o data loaader para processamento em mini-batchs\n",
    "- *Vetor de perguntas - Numpy*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6450280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_to_bert(np_sentences, tokenizer, sentence_max_length=450, batch_size=5):\n",
    "    inputs = tokenizer.batch_encode_plus(np_sentences, \n",
    "                                         return_tensors='pt',\n",
    "                                         padding='max_length',\n",
    "                                         max_length=sentence_max_length)\n",
    "    \n",
    "    input_ids = inputs['input_ids']\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    attention_mask = attention_mask.to('cuda')\n",
    "    \n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return loader   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d1937",
   "metadata": {},
   "source": [
    "##### Realiza o encode da sentença, retornando mean e mean polling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3726605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentences, bert_model, bert_tokenizer):\n",
    "    sentence_loader = prepare_to_bert(sentences, bert_tokenizer, sentence_max_length=450, batch_size=5)\n",
    "\n",
    "    mean_pooled_vet = []\n",
    "    mean_vet = []\n",
    "    \n",
    "    for batch_id, (input_ids, attention_masks) in enumerate(sentence_loader):\n",
    "        output = bert_model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "        embeddings = output.last_hidden_state\n",
    "        embeddings.detach()\n",
    "\n",
    "        # calculo mean_pooling\n",
    "        mask = attention_masks.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "        masked_embeddings = embeddings * mask\n",
    "        summed = torch.sum(masked_embeddings, 1)\n",
    "        summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "        mean_pooled = summed / summed_mask\n",
    "        # convert from PyTorch tensor to numpy array\n",
    "        mean_pooled = mean_pooled.cpu().data.numpy()\n",
    "        mean_pooled_vet.append(mean_pooled)\n",
    "\n",
    "        #calculo mean\n",
    "        mean = embeddings.mean(dim=1)\n",
    "        mean = mean.cpu().data.numpy()\n",
    "        mean_vet.append(mean)\n",
    "\n",
    "        #--- libera recursos da gpu\n",
    "        del output\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    mean_pooled_vet = np.concatenate(mean_pooled_vet)\n",
    "    mean_vet = np.concatenate(mean_vet)\n",
    "    \n",
    "    return mean_pooled_vet, mean_vet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "942e7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dada a base agrupada gera os embeddings por pergunta do contexto\n",
    "def gera_embedding_pergunta(qa_agrupada_assunto):\n",
    "    model, tokenizer = create_model(AppConstants.MODEL_BASE_NLI_MEAN)\n",
    "\n",
    "    with  tqdm(total=len(qa_agrupada_assunto), position=0, desc='Encoding de Sentenças') as progress:\n",
    "        for k_grupo in qa_agrupada_assunto:\n",
    "            progress.set_description(f\"Gerando Embedding Sentenças [{k_grupo:<30}]\")\n",
    "            grupo = qa_agrupada_assunto[k_grupo]\n",
    "            vet_pergunta = []\n",
    "            for q_embedding in grupo.question_list:\n",
    "                vet_pergunta.append(q_embedding.question)\n",
    "            sentence_embeddings = encode_sentence(vet_pergunta, bert_model=model, bert_tokenizer=tokenizer)    \n",
    "            for index, q_embedding in enumerate(grupo.question_list):\n",
    "                q_embedding.embedding = sentence_embeddings[index]\n",
    "            progress.update(1)\n",
    "        progress.set_description(\"Geração Embedding Finalizada\")\n",
    "    destroy_model(model_sbert_sem_finetunnig)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869282f",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fde9c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assuntos carregados.: 100%|██████████████████████████████████████████████████████████| 105/105 [01:30<00:00,  1.16it/s]\n",
      "Gerando Embedding Sentenças [abraham lincoln               ]:   0%|                            | 0/105 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-366a8867d958>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mqa_agrupada_assunto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magrupa_perguntas_por_assunto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvet_QA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcarrega_contexto_assunto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqa_agrupada_assunto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgera_embedding_pergunta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqa_agrupada_assunto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-133027c4d743>\u001b[0m in \u001b[0;36mgera_embedding_pergunta\u001b[1;34m(qa_agrupada_assunto)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0msentence_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvet_pergunta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_embedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrupo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0mq_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprogress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Geração Embedding Finalizada\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "vet_QA = carrega_dataset()\n",
    "qa_agrupada_assunto = agrupa_perguntas_por_assunto(vet_QA)\n",
    "carrega_contexto_assunto(qa_agrupada_assunto)\n",
    "gera_embedding_pergunta(qa_agrupada_assunto)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3932eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
