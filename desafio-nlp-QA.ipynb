{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6aa4d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2611c",
   "metadata": {},
   "source": [
    "##### Declaração das constantes utilizadas no código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a91f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset\"\n",
    "CONTEXT_TEXT_PATH = os.path.join(DATASET_PATH,\"text_data\")\n",
    "\n",
    "S08_QA = [\"S08_question_answer_pairs.txt\"]\n",
    "S09_QA = [\"S09_question_answer_pairs.txt\"]\n",
    "S10_QA = [\"S10_question_answer_pairs.txt\"]\n",
    "ALL_QA = S08_QA + S09_QA + S10_QA\n",
    "\n",
    "PUNCTUATIONS_TO_REMOVE = [\".\", \"!\", \"?\"]\n",
    "BAD_ANSWER_TO_REMOVE = [\"blah\",\"blah blah blah\",\"(What?)\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aaa457a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrega conteúdo dos arquivos com o texto utilizado para responder as perguntas\n",
    "def get_context_files():\n",
    "    context_file_map = {}\n",
    "    for f in os.listdir(CONTEXT_TEXT_PATH):\n",
    "        file_full_path = os.path.join(CONTEXT_TEXT_PATH, f)\n",
    "        if os.path.isfile(file_full_path):\n",
    "            with open(file_full_path, 'r', encoding=\"utf-8\") as f_contexto:\n",
    "                contexto = f_contexto.read().replace('\\n', '')\n",
    "            chave = f.replace(\".txt.clean\",\"\")\n",
    "            context_file_map[chave] = contexto\n",
    "    return context_file_map\n",
    "    \n",
    "#context_files = get_context_files()\n",
    "#  lista_tamanhos = []\n",
    "#  for cf in context_files:\n",
    "#       lista_tamanhos.append(len(context_files[cf].split()))\n",
    "#   print(lista_tamanhos[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682452ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_pairs(qa_source=ALL_QA):\n",
    "    df_list = []\n",
    "    for qa_file in qa_source:\n",
    "        df= pd.read_csv(os.path.join(DATASET_PATH,qa_file),sep='\\t')\n",
    "        df_list.append(df)\n",
    "    df_concat = pd.concat(df_list)\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial = load_qa_pairs(qa_source=S08_QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cddcddd",
   "metadata": {},
   "source": [
    "##### Análise inicial, somente com as primeiras 10 linhas:\n",
    "- Case diversos - deve ser tratado.\n",
    "- Algumas respostas tem pontuação - deve ser tratado.\n",
    "- Algumas perguntas não incluem o contexto - Did his mother die of pneumonia?. Validar se quando o title não aparecer no texto ele deve ser incluído."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7940b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6be6f9",
   "metadata": {},
   "source": [
    "##### Missing Values\n",
    "- Retirar perguntas nulas\n",
    "- Retirar respostas nulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd63892",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_inicial.isna().sum())\n",
    "print(df_inicial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar se existe alguma inconsistencia como resposta para pergunta nual\n",
    "df_inicial[(df_inicial.Question.isna()) &\n",
    "           (df_inicial.Answer.notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e18523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial[df_inicial.ArticleFile.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retirando missing values\n",
    "df_preparado = df_inicial[(df_inicial.Question.notna()) &\n",
    "                          (df_inicial.Answer.notna())]\n",
    "print(df_preparado.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preparado.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60969bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preparado.Answer.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030b349",
   "metadata": {},
   "source": [
    "#### Preparação dos dados\n",
    "\n",
    "Questões para tratamento\n",
    "- Lowercase\n",
    "- Pontuação da resposta, ultimo caracter com . ou ! ou ? devem ser retirados.\n",
    "- Pontuação da pergunta, devemos tirar a interrogação?\n",
    "- Retirar perguntas duplicadas: Respostas null já forma retiradas no pandas. Escolhe sempre a primeira.\n",
    "- Remover respostas ruins: blah, (What)?, blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5acd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_qa(vet_qa):\n",
    "    vet_n_resp = []\n",
    "    for qa in vet_qa:\n",
    "        pergunta, resposta = qa[0], qa[1]\n",
    "        pergunta = pergunta.lower()\n",
    "        \n",
    "        if (resposta[-1] in PUNCTUATIONS_TO_REMOVE):\n",
    "            resposta = resposta[:-1]\n",
    "        vet_n_resp.append([pergunta, resposta])\n",
    "    return np.array(vet_n_resp, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a513639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pergunta_igual_from_position(pos, vet, pergunta):\n",
    "    vet_resp = []\n",
    "    for index in range(pos, len(vet)):\n",
    "        if vet[index][0] == pergunta:\n",
    "            vet_resp.append(vet[index])\n",
    "    return vet_resp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "12575cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplication(vet_qa):\n",
    "    pergunta_set = set()\n",
    "    vet_qa_clean = []\n",
    "    \n",
    "    for pos,qa in enumerate(vet_qa):\n",
    "        pergunta, resposta = qa[0], qa[1]\n",
    "        if pergunta in pergunta_set:\n",
    "            continue\n",
    "        #perguntas_duplicadas = get_pergunta_igual_from_position(pos+1, vet_qa, pergunta)\n",
    "        pergunta_set.add(pergunta)\n",
    "        vet_qa_clean.append(qa)\n",
    "    return np.array(vet_qa_clean, dtype=object)            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ca59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_answer(vet_qa):\n",
    "    vet_qa_resp = []\n",
    "    for qa in vet_qa:\n",
    "        pergunta, resposta = qa[0], qa[1]\n",
    "        if pergunta in BAD\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b793bad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c1efb3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes remoção duplicadas (1475, 2)\n",
      "Após remoção duplicadas (916, 2)\n"
     ]
    }
   ],
   "source": [
    "vet_QA = df_preparado[[\"Question\", \"Answer\"]].to_numpy()\n",
    "vet_QA = normalize_qa(vet_QA)\n",
    "print('Antes remoção duplicadas', vet_QA.shape)\n",
    "vet_QA = remove_bad_answer(vet_QA)\n",
    "print('Após remoção duplicadas', vet_QA.shape)\n",
    "vet_QA = remove_duplication(vet_QA)\n",
    "print('Após remoção duplicadas', vet_QA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b72c1653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['was abraham lincoln the sixteenth president of the united states?',\n",
       "        'yes'],\n",
       "       ['did lincoln sign the national banking act of 1863?', 'yes'],\n",
       "       ['did his mother die of pneumonia?', 'no'],\n",
       "       [\"how many long was lincoln's formal education?\", '18 months'],\n",
       "       ['when did lincoln begin his political career?', '1832'],\n",
       "       ['what did the legal tender act of 1862 establish?',\n",
       "        'the United States Note, the first paper currency in United States history'],\n",
       "       ['who suggested lincoln grow a beard?',\n",
       "        '11-year-old Grace Bedell'],\n",
       "       ['when did the gettysburg address argue that america was born?',\n",
       "        '1776'],\n",
       "       ['did lincoln beat john c. breckinridge in the 1860 election?',\n",
       "        'yes'],\n",
       "       ['was abraham lincoln the first president of the united states?',\n",
       "        'No'],\n",
       "       ['did lincoln start his political career in 1832?', 'Yes'],\n",
       "       ['did lincoln ever represent alton & sangamon railroad?', 'Yes'],\n",
       "       ['which county was lincoln born in?', 'Hardin County'],\n",
       "       ['when did lincoln first serve as president?', 'March 4, 1861'],\n",
       "       ['who assassinated lincoln?', 'John Wilkes Booth'],\n",
       "       ['did lincoln win the election of 1860?', 'Yes'],\n",
       "       ['who was the general in charge at the battle of antietam?',\n",
       "        'General McClellan'],\n",
       "       ['why did lincoln issue the emancipation proclamation?',\n",
       "        'To free slaves '],\n",
       "       ['do scholars rank lincoln among the top three presidents?',\n",
       "        'Yes'],\n",
       "       ['did lincoln have 18 months of schooling?', 'Yes'],\n",
       "       ['was lincoln chosen as a presidential candidate in 1860?', 'Yes'],\n",
       "       ['how old was lincoln in 1816?', 'seven'],\n",
       "       ['when was the first photgraph of lincoln taken?', '1846'],\n",
       "       [\"how long was lincoln's legal career?\", '23 years'],\n",
       "       [\"what trail did lincoln use a farmers' almanac in? \",\n",
       "        'he defended William \"Duff\" Armstrong'],\n",
       "       ['did abraham lincoln live in the frontier?', 'Yes'],\n",
       "       [\"did lincoln's wife's family support slavery?\", 'Yes'],\n",
       "       ['who is most noted for his contributions to the theory of molarity and molecular weight?',\n",
       "        'Amedeo Avogadro'],\n",
       "       ['who graduated in ecclesiastical law at the early age of 20 and began to practice?',\n",
       "        'Amedeo Avogadro'],\n",
       "       ['when did he publish another memoria?', '1821'],\n",
       "       ['when did he become a professor?', '1820'],\n",
       "       ['is it true that he became a professor in 1820?', 'yes'],\n",
       "       ['was lorenzo romano amedeo carlo avogadro an italian savant?',\n",
       "        'yes'],\n",
       "       ['was amedeo avogadro born in turin august 9th 1776 to a noble ancient family of piedmont, italy?',\n",
       "        'yes'],\n",
       "       ['is he most noted for his contributions to the theory of molarity and molecular weight?',\n",
       "        'yes'],\n",
       "       ['was king victor emmanuel iii there to pay homage to avogadro ?',\n",
       "        'yes'],\n",
       "       [\"is avogadro 's number commonly used to compute the results of chemical reactions ?\",\n",
       "        'yes'],\n",
       "       ['did the scientific community not reserve great attention to his theory ?',\n",
       "        'yes'],\n",
       "       ['can the title of this famous 1811 paper be roughly translated into english?',\n",
       "        'yes'],\n",
       "       ['what happened in 1833?',\n",
       "        'Avogadro had been recalled to Turin university'],\n",
       "       ['who determined the dependence of the boiling of water with atmospheric pressure?',\n",
       "        'Anders Celsius'],\n",
       "       ['what is named after him?', 'The Celsius crater on the Moon'],\n",
       "       ['when did he publish a collection?', '1733'],\n",
       "       ['is it true that he published a collection in 1738?', 'No'],\n",
       "       ['is it true that thermometer had 100 for the freezing point?',\n",
       "        'Yes'],\n",
       "       ['was celsius born in uppsala in sweden?', 'Yes'],\n",
       "       ['was anders celsius (november 27, 1701 april 25, 1744) a swedish astronomer?',\n",
       "        'Yes'],\n",
       "       ['is the celsius crater on the moon named after him?', 'Yes'],\n",
       "       ['who was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds ?',\n",
       "        'Anders Celsius'],\n",
       "       ['the celsius crater on the moon is what?', 'named after him']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vet_QA[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc205a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
